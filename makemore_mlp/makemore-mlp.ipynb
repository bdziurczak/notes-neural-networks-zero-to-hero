{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-18T16:06:40.842278Z",
          "iopub.status.busy": "2024-05-18T16:06:40.841859Z",
          "iopub.status.idle": "2024-05-18T16:06:40.848661Z",
          "shell.execute_reply": "2024-05-18T16:06:40.847592Z",
          "shell.execute_reply.started": "2024-05-18T16:06:40.842248Z"
        },
        "id": "VN6h4B429WxI",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-18T16:06:40.851045Z",
          "iopub.status.busy": "2024-05-18T16:06:40.850714Z",
          "iopub.status.idle": "2024-05-18T16:06:40.883871Z",
          "shell.execute_reply": "2024-05-18T16:06:40.882903Z",
          "shell.execute_reply.started": "2024-05-18T16:06:40.851020Z"
        },
        "id": "PwA_tbI09WxX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-18T16:06:40.885442Z",
          "iopub.status.busy": "2024-05-18T16:06:40.885155Z",
          "iopub.status.idle": "2024-05-18T16:06:40.914976Z",
          "shell.execute_reply": "2024-05-18T16:06:40.914142Z",
          "shell.execute_reply.started": "2024-05-18T16:06:40.885419Z"
        },
        "id": "M2MUejNT9Wxg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "words = open('../data/names.txt', 'r').read().splitlines()\n",
        "\n",
        "# vocabulary of characters and mapping to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-18T16:06:40.916972Z",
          "iopub.status.busy": "2024-05-18T16:06:40.916672Z",
          "iopub.status.idle": "2024-05-18T16:06:41.900238Z",
          "shell.execute_reply": "2024-05-18T16:06:41.899172Z",
          "shell.execute_reply.started": "2024-05-18T16:06:40.916949Z"
        },
        "id": "VVzMEt8d9Wxm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "block_size = 3 #how many character are taken to predict next one\n",
        "def build_dataset(words):\n",
        "    inputs, labels = [],[]\n",
        "\n",
        "    for w in words:\n",
        "        context = [0] * block_size\n",
        "        for ch in w + '.':\n",
        "            ix = stoi[ch]\n",
        "            inputs.append(context)\n",
        "            labels.append(ix)\n",
        "            context = context[1:] + [ix] #crop and append next character\n",
        "    inputs = torch.tensor(inputs).to(dev)\n",
        "    labels = torch.tensor(labels).to(dev)\n",
        "    return inputs, labels\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "xtrain, ytrain = build_dataset(words[:n1])\n",
        "xdev, ydev = build_dataset(words[n1:n2])\n",
        "xtest, ytest = build_dataset(words[n2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bwV1Auol9WyN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46497\n"
          ]
        }
      ],
      "source": [
        "class Linear:\n",
        "    def __init__(self, fan_in, fan_out, bias=True):\n",
        "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
        "        self.bias = torch.zeros(fan_out) if bias else None\n",
        "    \n",
        "\n",
        "    def __call__(self, x):\n",
        "        #data -> layer is like data @ layer\n",
        "        self.out = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            self.out += self.bias\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "    \n",
        "class BatchNorm1d:\n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.training = True\n",
        "        #parameters trained with backpropagation\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "        #buffers (trained with a running 'momentum update')\n",
        "        self.running_mean = torch.zeros(dim)\n",
        "        self.running_var = torch.ones(dim)\n",
        "    def __call__(self, x):\n",
        "        if self.training:\n",
        "            xmean = x.mean(0, keepdim=True)\n",
        "            xvar = x.var(0, keepdim=True, unbiased=True)\n",
        "        else:\n",
        "            xmean = self.running_mean\n",
        "            xvar = self.running_var\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "\n",
        "        if self.training:\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "        return self.out\n",
        "    \n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "    \n",
        "class Tanh:\n",
        "    def __call__(self, x):\n",
        "        self.out = torch.tanh(x)\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        return []\n",
        "    \n",
        "\n",
        "ch_embed_dim = 10\n",
        "n_hidden = 100\n",
        "\n",
        "C = torch.randn((vocab_size, ch_embed_dim))\n",
        "layers = [\n",
        "    Linear(ch_embed_dim * block_size, n_hidden), Tanh(),\n",
        "    Linear(n_hidden, n_hidden), Tanh(),\n",
        "    Linear(n_hidden, n_hidden), Tanh(),\n",
        "    Linear(n_hidden, n_hidden), Tanh(),\n",
        "    Linear(n_hidden, n_hidden), Tanh(),\n",
        "    Linear(n_hidden, vocab_size),\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    #make last layer less confindent \n",
        "    layers[-1].weight *= 0.1\n",
        "    #for all other layers apply gain\n",
        "    for layer in layers[:-1]:\n",
        "        if isinstance(layer, Linear):\n",
        "            layer.weight *= 5/3\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters: \n",
        "    p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      0/ 200000: 3.3070\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m embed\u001b[38;5;241m.\u001b[39mview(embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(x, ybatch)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#backward pass\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#optimization\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "for i in range(max_steps):\n",
        "    #construct batch\n",
        "    ix = torch.randint(0, xtrain.shape[0], (batch_size,))\n",
        "    xbatch, ybatch = xtrain[ix], ytrain[ix]\n",
        "\n",
        "    #forward pass \n",
        "    embed = C[xbatch]\n",
        "    x = embed.view(embed.shape[0], -1)\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "    loss = F.cross_entropy(x, ybatch)\n",
        "\n",
        "    #backward pass\n",
        "    for layer in layers:\n",
        "        layer.out.retain_grad()\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    learn_rate = 0.1 if i < 100000 else 0.01\n",
        "    for p in parameters:\n",
        "        p.data -= learn_rate * p.grad\n",
        "    \n",
        "    if i % 10000 == 0:\n",
        "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4949276,
          "sourceId": 8334180,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
